{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Notes using Random Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import pretty_midi\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from tqdm import tqdm_notebook\n",
    "from numpy.random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqSelfAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    ATTENTION_TYPE_ADD = 'additive'\n",
    "    ATTENTION_TYPE_MUL = 'multiplicative'\n",
    "\n",
    "    def __init__(self,\n",
    "                 units=32,\n",
    "                 attention_width=None,\n",
    "                 attention_type=ATTENTION_TYPE_ADD,\n",
    "                 return_attention=False,\n",
    "                 history_only=False,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 use_additive_bias=True,\n",
    "                 use_attention_bias=True,\n",
    "                 attention_activation=None,\n",
    "                 attention_regularizer_weight=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Layer initialization.\n",
    "        For additive attention, see: https://arxiv.org/pdf/1806.01264.pdf\n",
    "        :param units: The dimension of the vectors that used to calculate the attention weights.\n",
    "        :param attention_width: The width of local attention.\n",
    "        :param attention_type: 'additive' or 'multiplicative'.\n",
    "        :param return_attention: Whether to return the attention weights for visualization.\n",
    "        :param history_only: Only use historical pieces of data.\n",
    "        :param kernel_initializer: The initializer for weight matrices.\n",
    "        :param bias_initializer: The initializer for biases.\n",
    "        :param kernel_regularizer: The regularization for weight matrices.\n",
    "        :param bias_regularizer: The regularization for biases.\n",
    "        :param kernel_constraint: The constraint for weight matrices.\n",
    "        :param bias_constraint: The constraint for biases.\n",
    "        :param use_additive_bias: Whether to use bias while calculating the relevance of inputs features\n",
    "                                  in additive mode.\n",
    "        :param use_attention_bias: Whether to use bias while calculating the weights of attention.\n",
    "        :param attention_activation: The activation used for calculating the weights of attention.\n",
    "        :param attention_regularizer_weight: The weights of attention regularizer.\n",
    "        :param kwargs: Parameters for parent class.\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.units = units\n",
    "        self.attention_width = attention_width\n",
    "        self.attention_type = attention_type\n",
    "        self.return_attention = return_attention\n",
    "        self.history_only = history_only\n",
    "        if history_only and attention_width is None:\n",
    "            self.attention_width = int(1e9)\n",
    "\n",
    "        self.use_additive_bias = use_additive_bias\n",
    "        self.use_attention_bias = use_attention_bias\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n",
    "        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n",
    "        self.attention_activation = tf.keras.activations.get(attention_activation)\n",
    "        self.attention_regularizer_weight = attention_regularizer_weight\n",
    "        self._backend = tf.keras.backend.backend()\n",
    "\n",
    "        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
    "            self.Wx, self.Wt, self.bh = None, None, None\n",
    "            self.Wa, self.ba = None, None\n",
    "        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
    "            self.Wa, self.ba = None, None\n",
    "        else:\n",
    "            raise NotImplementedError('No implementation for attention type : ' + attention_type)\n",
    "\n",
    "        super(SeqSelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'attention_width': self.attention_width,\n",
    "            'attention_type': self.attention_type,\n",
    "            'return_attention': self.return_attention,\n",
    "            'history_only': self.history_only,\n",
    "            'use_additive_bias': self.use_additive_bias,\n",
    "            'use_attention_bias': self.use_attention_bias,\n",
    "            'kernel_initializer': tf.keras.regularizers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': tf.keras.regularizers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': tf.keras.regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': tf.keras.regularizers.serialize(self.bias_regularizer),\n",
    "            'kernel_constraint': tf.keras.constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': tf.keras.constraints.serialize(self.bias_constraint),\n",
    "            'attention_activation': tf.keras.activations.serialize(self.attention_activation),\n",
    "            'attention_regularizer_weight': self.attention_regularizer_weight,\n",
    "        }\n",
    "        base_config = super(SeqSelfAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape = input_shape[0]\n",
    "        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
    "            self._build_additive_attention(input_shape)\n",
    "        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
    "            self._build_multiplicative_attention(input_shape)\n",
    "        super(SeqSelfAttention, self).build(input_shape)\n",
    "\n",
    "    def _build_additive_attention(self, input_shape):\n",
    "        feature_dim = input_shape[2]\n",
    "\n",
    "        self.Wt = self.add_weight(shape=(feature_dim, self.units),\n",
    "                                  name='{}_Add_Wt'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        self.Wx = self.add_weight(shape=(feature_dim, self.units),\n",
    "                                  name='{}_Add_Wx'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        if self.use_additive_bias:\n",
    "            self.bh = self.add_weight(shape=(self.units,),\n",
    "                                      name='{}_Add_bh'.format(self.name),\n",
    "                                      initializer=self.bias_initializer,\n",
    "                                      regularizer=self.bias_regularizer,\n",
    "                                      constraint=self.bias_constraint)\n",
    "\n",
    "        self.Wa = self.add_weight(shape=(self.units, 1),\n",
    "                                  name='{}_Add_Wa'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        if self.use_attention_bias:\n",
    "            self.ba = self.add_weight(shape=(1,),\n",
    "                                      name='{}_Add_ba'.format(self.name),\n",
    "                                      initializer=self.bias_initializer,\n",
    "                                      regularizer=self.bias_regularizer,\n",
    "                                      constraint=self.bias_constraint)\n",
    "\n",
    "    def _build_multiplicative_attention(self, input_shape):\n",
    "        feature_dim = input_shape[2]\n",
    "\n",
    "        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),\n",
    "                                  name='{}_Mul_Wa'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        if self.use_attention_bias:\n",
    "            self.ba = self.add_weight(shape=(1,),\n",
    "                                      name='{}_Mul_ba'.format(self.name),\n",
    "                                      initializer=self.bias_initializer,\n",
    "                                      regularizer=self.bias_regularizer,\n",
    "                                      constraint=self.bias_constraint)\n",
    "\n",
    "    def call(self, inputs, mask=None, **kwargs):\n",
    "        if isinstance(inputs, list):\n",
    "            inputs, positions = inputs\n",
    "            positions = K.cast(positions, 'int32')\n",
    "            mask = mask[1]\n",
    "        else:\n",
    "            positions = None\n",
    "\n",
    "        input_len = K.shape(inputs)[1]\n",
    "\n",
    "        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
    "            e = self._call_additive_emission(inputs)\n",
    "        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
    "            e = self._call_multiplicative_emission(inputs)\n",
    "\n",
    "        if self.attention_activation is not None:\n",
    "            e = self.attention_activation(e)\n",
    "        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n",
    "        if self.attention_width is not None:\n",
    "            ones = tf.ones((input_len, input_len))\n",
    "            if self.history_only:\n",
    "                local = tf.linalg.band_part(\n",
    "                    ones,\n",
    "                    K.minimum(input_len, self.attention_width - 1),\n",
    "                    0,\n",
    "                )\n",
    "            else:\n",
    "                local = tf.linalg.band_part(\n",
    "                    ones,\n",
    "                    K.minimum(input_len, self.attention_width // 2),\n",
    "                    K.minimum(input_len, (self.attention_width - 1) // 2),\n",
    "                )\n",
    "            e = e * K.expand_dims(local, 0)\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask)\n",
    "            e = K.permute_dimensions(K.permute_dimensions(e * mask, (0, 2, 1)) * mask, (0, 2, 1))\n",
    "\n",
    "        # a_{t} = \\text{softmax}(e_t)\n",
    "        s = K.sum(e, axis=-1)\n",
    "        s = K.tile(K.expand_dims(s, axis=-1), K.stack([1, 1, input_len]))\n",
    "        a = e / (s + K.epsilon())\n",
    "\n",
    "        # l_t = \\sum_{t'} a_{t, t'} x_{t'}\n",
    "        v = K.batch_dot(a, inputs)\n",
    "        if self.attention_regularizer_weight > 0.0:\n",
    "            self.add_loss(self._attention_regularizer(a))\n",
    "\n",
    "        if positions is not None:\n",
    "            pos_num = K.shape(positions)[1]\n",
    "            batch_indices = K.tile(K.expand_dims(K.arange(K.shape(inputs)[0]), axis=-1), K.stack([1, pos_num]))\n",
    "            pos_indices = K.stack([batch_indices, positions], axis=-1)\n",
    "            v = tf.gather_nd(v, pos_indices)\n",
    "            a = tf.gather_nd(a, pos_indices)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [v, a]\n",
    "        return v\n",
    "\n",
    "    def _call_additive_emission(self, inputs):\n",
    "        input_shape = K.shape(inputs)\n",
    "        batch_size, input_len = input_shape[0], input_shape[1]\n",
    "\n",
    "        # h_{t, t'} = \\tanh(x_t^T W_t + x_{t'}^T W_x + b_h)\n",
    "        q, k = K.dot(inputs, self.Wt), K.dot(inputs, self.Wx)\n",
    "        q = K.tile(K.expand_dims(q, 2), K.stack([1, 1, input_len, 1]))\n",
    "        k = K.tile(K.expand_dims(k, 1), K.stack([1, input_len, 1, 1]))\n",
    "        if self.use_additive_bias:\n",
    "            h = K.tanh(q + k + self.bh)\n",
    "        else:\n",
    "            h = K.tanh(q + k)\n",
    "\n",
    "        # e_{t, t'} = W_a h_{t, t'} + b_a\n",
    "        if self.use_attention_bias:\n",
    "            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))\n",
    "        else:\n",
    "            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))\n",
    "        return e\n",
    "\n",
    "    def _call_multiplicative_emission(self, inputs):\n",
    "        # e_{t, t'} = x_t^T W_a x_{t'} + b_a\n",
    "        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))\n",
    "        if self.use_attention_bias:\n",
    "            e = e + self.ba\n",
    "        return e\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape, pos_shape = input_shape\n",
    "            output_shape = (input_shape[0], pos_shape[1], input_shape[2])\n",
    "        else:\n",
    "            output_shape = input_shape\n",
    "        if self.return_attention:\n",
    "            attention_shape = (input_shape[0], output_shape[1], input_shape[1])\n",
    "            return [output_shape, attention_shape]\n",
    "        return output_shape\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if isinstance(inputs, list):\n",
    "            mask = mask[1]\n",
    "        if self.return_attention:\n",
    "            return [mask, None]\n",
    "        return mask\n",
    "\n",
    "    def _attention_regularizer(self, attention):\n",
    "        batch_size = K.cast(K.shape(attention)[0], K.floatx())\n",
    "        input_len = K.shape(attention)[-1]\n",
    "        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(\n",
    "            attention,\n",
    "            K.permute_dimensions(attention, (0, 2, 1))) - tf.eye(input_len))) / batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def get_custom_objects():\n",
    "        return {'SeqSelfAttention': SeqSelfAttention}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piano_roll_to_pretty_midi(piano_roll, fs=100, program=0):\n",
    "    '''Convert a Piano Roll array into a PrettyMidi object\n",
    "     with a single instrument.\n",
    "    Parameters\n",
    "    ----------\n",
    "    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n",
    "        Piano roll of one instrument\n",
    "    fs : int\n",
    "        Sampling frequency of the columns, i.e. each column is spaced apart\n",
    "        by ``1./fs`` seconds.\n",
    "    program : int\n",
    "        The program number of the instrument.\n",
    "    Returns\n",
    "    -------\n",
    "    midi_object : pretty_midi.PrettyMIDI\n",
    "        A pretty_midi.PrettyMIDI class instance describing\n",
    "        the piano roll.\n",
    "    '''\n",
    "    notes, frames = piano_roll.shape\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(program=program)\n",
    "\n",
    "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
    "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
    "\n",
    "    # use changes in velocities to find note on / note off events\n",
    "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
    "\n",
    "    # keep track on velocities and note on times\n",
    "    prev_velocities = np.zeros(notes, dtype=int)\n",
    "    note_on_time = np.zeros(notes)\n",
    "\n",
    "    for time, note in zip(*velocity_changes):\n",
    "        # use time + 1 because of padding above\n",
    "        velocity = piano_roll[note, time + 1]\n",
    "        time = time / fs\n",
    "        if velocity > 0:\n",
    "            if prev_velocities[note] == 0:\n",
    "                note_on_time[note] = time\n",
    "                prev_velocities[note] = velocity\n",
    "        else:\n",
    "            pm_note = pretty_midi.Note(\n",
    "                velocity=prev_velocities[note],\n",
    "                pitch=note,\n",
    "                start=note_on_time[note],\n",
    "                end=time)\n",
    "            instrument.notes.append(pm_note)\n",
    "            prev_velocities[note] = 0\n",
    "    pm.instruments.append(instrument)\n",
    "    return pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_target = []\n",
    "with open('data/sample_input.json', 'r') as handle:\n",
    "    for line in handle:\n",
    "        song = json.loads(line)\n",
    "        sample_target.append(song['target'])\n",
    "notes = [y for x in sample_target for y in x]\n",
    "unique_notes = len(set(notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_random(unique_notes, seq_len=50):\n",
    "    generate = np.random.randint(0,unique_notes,seq_len).tolist()\n",
    "    return generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(generate, model, unique_notes, max_generated=1000, seq_len=50):\n",
    "    for i in tqdm_notebook(range(max_generated), desc='genrt'):\n",
    "        test_input = np.array([generate])[:,i:i+seq_len]\n",
    "        predicted_note = model.predict(test_input)\n",
    "        random_note_pred = choice(unique_notes, 1, replace=False, p=predicted_note[0])\n",
    "        generate.append(random_note_pred[0])\n",
    "    return generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/note_index_40.json', 'r') as read_file:\n",
    "    dict_index = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_midi_file_from_generated(generate, dict_index,midi_file_name = \"result.mid\", start_index=49, fs=8, max_generated=1000):\n",
    "    note_string = []\n",
    "    for note, ind in dict_index.items():\n",
    "        for ind_note in generate:\n",
    "            if ind_note == ind:\n",
    "                note_string.append(note)\n",
    "    print(note_string)\n",
    "    array_piano_roll = np.zeros((128,max_generated+1), dtype=np.int16)\n",
    "    for index, note in enumerate(note_string[start_index:]):\n",
    "        if note == 'e':\n",
    "            pass\n",
    "        else:\n",
    "            splitted_note = note.split(',')\n",
    "            for j in splitted_note:\n",
    "                array_piano_roll[int(j),index] = 1\n",
    "    generate_to_midi = piano_roll_to_pretty_midi(array_piano_roll, fs=fs)\n",
    "    print(\"Tempo {}\".format(generate_to_midi.estimate_tempo()))\n",
    "    for note in generate_to_midi.instruments[0].notes:\n",
    "        note.velocity = 100\n",
    "    generate_to_midi.write(midi_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model_s40_ep2.h5', custom_objects=SeqSelfAttention.get_custom_objects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43630"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2159b1e989d3415bb8df881a2ff72df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='genrt', max=200, style=ProgressStyle(description_width='initiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_generate = 200\n",
    "seq_len=50\n",
    "generate = generate_from_random(unique_notes, seq_len)\n",
    "generate = generate_notes(generate, model, unique_notes, max_generate, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e': 0,\n",
       " '38,50,78': 1,\n",
       " '38,78': 2,\n",
       " '38,69,78': 3,\n",
       " '38,43,78': 4,\n",
       " '38,43,71,78': 5,\n",
       " '38,43,71': 6,\n",
       " '38,71': 7,\n",
       " '38,67,71': 8,\n",
       " '38,67': 9,\n",
       " '38': 10,\n",
       " '38,49': 11,\n",
       " '38,49,76': 12,\n",
       " '38,76': 13,\n",
       " '38,71,72': 14,\n",
       " '38,72': 15,\n",
       " '38,74': 16,\n",
       " '38,42,84': 17,\n",
       " '38,84': 18,\n",
       " '38,83,84': 19,\n",
       " '38,83': 20,\n",
       " '38,81,83': 21,\n",
       " '38,81': 22,\n",
       " '38,81,84': 23,\n",
       " '38,69': 24,\n",
       " '38,50,71': 25,\n",
       " '38,50,67,69': 26,\n",
       " '38,67,69': 27,\n",
       " '38,40,66,69': 28,\n",
       " '38,66,69': 29,\n",
       " '38,66,72': 30,\n",
       " '38,66': 31,\n",
       " '38,66,69,71': 32,\n",
       " '38,66,71': 33,\n",
       " '38,66,69,74': 34,\n",
       " '38,69,74': 35,\n",
       " '38,40': 36,\n",
       " '38,50,66,71': 37,\n",
       " '38,50,66,72': 38,\n",
       " '38,48,66,76': 39,\n",
       " '38,66,76': 40,\n",
       " '38,45,66': 41,\n",
       " '38,71,74': 42,\n",
       " '38,62': 43,\n",
       " '38,42,69,74': 44,\n",
       " '38,42,74': 45,\n",
       " '38,45,66,74': 46,\n",
       " '38,45,74': 47,\n",
       " '38,74,76': 48,\n",
       " '38,62,72': 49,\n",
       " '38,42,62,72': 50,\n",
       " '38,50,69': 51,\n",
       " '38,50,60,66': 52,\n",
       " '38,50,60,66,69': 53,\n",
       " '40,52,79': 54,\n",
       " '40,52,75': 55,\n",
       " '40': 56,\n",
       " '40,67': 57,\n",
       " '40,52': 58,\n",
       " '40,52,67': 59,\n",
       " '40,71': 60,\n",
       " '40,69': 61,\n",
       " '40,71,76': 62,\n",
       " '40,76': 63,\n",
       " '40,81': 64,\n",
       " '40,79,81': 65,\n",
       " '40,79': 66,\n",
       " '40,78,79': 67,\n",
       " '40,78': 68,\n",
       " '40,55,76': 69,\n",
       " '40,67,71': 70,\n",
       " '40,42': 71,\n",
       " '40,42,81': 72,\n",
       " '40,42,74,81': 73,\n",
       " '40,66,69': 74,\n",
       " '40,62': 75,\n",
       " '40,62,67': 76,\n",
       " '40,42,67,69': 77,\n",
       " '40,64': 78,\n",
       " '40,61,67': 79,\n",
       " '40,42,69': 80,\n",
       " '40,57': 81,\n",
       " '40,55': 82,\n",
       " '40,59': 83,\n",
       " '40,74': 84,\n",
       " '40,67,74': 85,\n",
       " '41': 86,\n",
       " '41,43,76': 87,\n",
       " '41,43': 88,\n",
       " '41,43,83': 89,\n",
       " '42,72': 90,\n",
       " '42,71,72': 91,\n",
       " '42,71': 92,\n",
       " '42,43,69': 93,\n",
       " '42,69': 94,\n",
       " '42': 95,\n",
       " '42,74': 96,\n",
       " '42,74,76': 97,\n",
       " '42,76': 98,\n",
       " '42,45': 99,\n",
       " '42,45,78': 100,\n",
       " '42,78': 101,\n",
       " '42,72,81': 102,\n",
       " '42,81': 103,\n",
       " '42,49,50,76,78': 104,\n",
       " '42,50,76,78': 105,\n",
       " '42,50,76': 106,\n",
       " '42,52,76': 107,\n",
       " '42,52,74': 108,\n",
       " '42,54,74': 109,\n",
       " '42,62': 110,\n",
       " '42,62,64': 111,\n",
       " '42,64': 112,\n",
       " '42,45,64,66': 113,\n",
       " '42,45,66': 114,\n",
       " '42,66': 115,\n",
       " '42,67': 116,\n",
       " '42,50,67,69': 117,\n",
       " '42,50,69': 118,\n",
       " '42,50,71,72': 119,\n",
       " '42,50,72': 120,\n",
       " '42,81,83': 121,\n",
       " '42,81,83,84': 122,\n",
       " '42,83,84': 123,\n",
       " '42,84': 124,\n",
       " '42,45,76,78': 125,\n",
       " '42,45,76': 126,\n",
       " '42,45,74,76': 127,\n",
       " '42,45,74': 128,\n",
       " '42,45,64': 129,\n",
       " '42,67,69': 130,\n",
       " '42,50,71': 131,\n",
       " '42,48,62,64': 132,\n",
       " '42,48,64': 133,\n",
       " '42,45,69,71': 134,\n",
       " '42,45,71': 135,\n",
       " '42,45,72': 136,\n",
       " '42,74,81': 137,\n",
       " '42,43': 138,\n",
       " '42,43,72,81': 139,\n",
       " '42,43,81': 140,\n",
       " '42,66,69': 141,\n",
       " '42,52,79': 142,\n",
       " '42,52,71,79': 143,\n",
       " '42,71,79': 144,\n",
       " '42,45,69,71,79': 145,\n",
       " '42,45,69,79': 146,\n",
       " '42,69,79': 147,\n",
       " '42,47,69,79': 148,\n",
       " '42,69,76,79': 149,\n",
       " '42,69,76': 150,\n",
       " '42,49,69,76': 151,\n",
       " '42,69,78': 152,\n",
       " '42,50,69,78': 153,\n",
       " '42,50,78': 154,\n",
       " '42,50': 155,\n",
       " '42,52': 156,\n",
       " '42,54,67,69': 157,\n",
       " '42,54,67': 158,\n",
       " '42,66,69,74': 159,\n",
       " '42,49,69,79': 160,\n",
       " '42,52,54,69': 161,\n",
       " '42,52,54,67,69': 162,\n",
       " '42,66,74': 163,\n",
       " '42,62,74': 164,\n",
       " '42,45,62': 165,\n",
       " '42,62,69': 166,\n",
       " '42,62,67': 167,\n",
       " '42,61': 168,\n",
       " '42,61,69': 169,\n",
       " '42,43,71': 170,\n",
       " '42,43,62': 171,\n",
       " '42,64,71': 172,\n",
       " '42,64,69,71': 173,\n",
       " '42,64,69': 174,\n",
       " '42,62,71': 175,\n",
       " '42,61,67': 176,\n",
       " '42,61,67,69': 177,\n",
       " '42,69,71': 178,\n",
       " '42,43,69,71': 179,\n",
       " '42,50,62': 180,\n",
       " '42,50,55,62': 181,\n",
       " '42,57': 182,\n",
       " '42,60': 183,\n",
       " '42,60,64': 184,\n",
       " '42,63': 185,\n",
       " '42,45,62,69': 186,\n",
       " '42,45,62,69,72': 187,\n",
       " '42,45,62,72': 188,\n",
       " '42,62,72': 189,\n",
       " '42,62,69,71': 190,\n",
       " '42,57,60': 191,\n",
       " '42,62,69,72': 192,\n",
       " '42,45,66,72': 193,\n",
       " '43,55,71': 194,\n",
       " '43,47,55,71': 195,\n",
       " '43,50,76,78': 196,\n",
       " '43,50,78': 197,\n",
       " '43': 198,\n",
       " '43,71': 199,\n",
       " '43,47': 200,\n",
       " '43,47,65': 201,\n",
       " '43,65': 202,\n",
       " '43,55,71,74': 203,\n",
       " '43,50,55,59,62,67': 204,\n",
       " '43,50,55,62,67': 205,\n",
       " '43,50,55,67': 206,\n",
       " '43,50,55': 207,\n",
       " '43,55': 208,\n",
       " '43,45,72,76': 209,\n",
       " '43,45,72': 210,\n",
       " '43,50,55,62': 211,\n",
       " '43,72': 212,\n",
       " '43,67': 213,\n",
       " '43,69': 214,\n",
       " '43,67,71': 215,\n",
       " '43,66,67,71': 216,\n",
       " '43,66,71': 217,\n",
       " '43,76': 218,\n",
       " '43,74': 219,\n",
       " '43,74,76': 220,\n",
       " '43,45,76': 221,\n",
       " '43,45': 222,\n",
       " '43,72,74,76': 223,\n",
       " '43,72,74': 224,\n",
       " '43,55,67,69': 225,\n",
       " '43,55,67': 226,\n",
       " '43,49,81': 227,\n",
       " '43,54,74': 228,\n",
       " '43,54,71,74': 229,\n",
       " '43,54,71': 230,\n",
       " '43,55,76': 231,\n",
       " '43,50,69': 232,\n",
       " '43,50': 233,\n",
       " '43,50,71': 234,\n",
       " '43,50,69,71': 235,\n",
       " '43,50,67': 236,\n",
       " '43,48,50,67': 237,\n",
       " '43,48,50,52,67': 238,\n",
       " '43,48,50,52': 239,\n",
       " '43,48,50,52,69': 240,\n",
       " '43,48,52,69': 241,\n",
       " '43,48,52,67': 242,\n",
       " '43,55,72': 243,\n",
       " '43,57,78': 244,\n",
       " '43,57,79': 245,\n",
       " '43,79': 246,\n",
       " '43,45,47': 247,\n",
       " '43,47,74': 248,\n",
       " '43,47,74,83': 249,\n",
       " '43,47,83': 250,\n",
       " '43,47,81': 251,\n",
       " '43,81': 252,\n",
       " '43,45,50,69': 253,\n",
       " '43,45,50,71': 254,\n",
       " '43,55,71,72': 255,\n",
       " '43,57,66': 256,\n",
       " '43,57,66,67': 257,\n",
       " '43,57,67': 258,\n",
       " '43,47,72': 259,\n",
       " '43,47,72,74': 260,\n",
       " '43,47,74,79': 261,\n",
       " '43,47,79': 262,\n",
       " '43,78,79': 263,\n",
       " '43,45,48,78': 264,\n",
       " '43,74,83': 265,\n",
       " '43,83': 266,\n",
       " '43,71,76': 267,\n",
       " '43,45,74': 268,\n",
       " '43,45,79': 269,\n",
       " '43,45,72,79': 270,\n",
       " '43,72,79': 271,\n",
       " '43,45,69,78': 272,\n",
       " '43,71,79': 273,\n",
       " '43,79,83': 274,\n",
       " '43,47,67': 275,\n",
       " '43,67,72': 276,\n",
       " '43,47,67,71': 277,\n",
       " '43,59,62,67': 278,\n",
       " '43,59,62,66': 279,\n",
       " '43,59,62,66,67': 280,\n",
       " '43,67,74': 281,\n",
       " '43,62,74': 282,\n",
       " '43,64': 283,\n",
       " '43,64,71': 284,\n",
       " '43,62,69': 285,\n",
       " '43,62,69,71': 286,\n",
       " '43,62,71': 287,\n",
       " '43,61,67': 288,\n",
       " '43,61,69': 289,\n",
       " '43,69,71': 290,\n",
       " '43,45,73': 291,\n",
       " '43,62': 292,\n",
       " '43,62,64': 293,\n",
       " '43,45,64': 294,\n",
       " '43,47,64,73': 295,\n",
       " '43,47,64': 296,\n",
       " '43,59,62': 297,\n",
       " '43,59': 298,\n",
       " '43,52,60': 299,\n",
       " '43,60': 300,\n",
       " '43,50,59': 301,\n",
       " '43,50,62': 302,\n",
       " '43,75,79,83': 303,\n",
       " '43,75,83': 304,\n",
       " '43,62,72': 305,\n",
       " '43,62,71,72': 306,\n",
       " '43,64,74': 307,\n",
       " '43,60,67': 308,\n",
       " '43,47,62,67': 309,\n",
       " '43,59,67': 310,\n",
       " '43,50,59,67': 311,\n",
       " '43,55,59': 312,\n",
       " '43,50,59,60': 313,\n",
       " '43,50,59,62': 314,\n",
       " '43,52,59,60': 315,\n",
       " '43,50,60,66': 316,\n",
       " '43,50,60': 317,\n",
       " '43,50,60,67': 318,\n",
       " '43,47,60,67': 319,\n",
       " '43,47,60,62,67': 320,\n",
       " '43,62,67': 321,\n",
       " '43,50,59,62,67': 322,\n",
       " '43,55,59,62,67': 323,\n",
       " '45,50,55,71': 324,\n",
       " '45,55,71': 325,\n",
       " '45,55,69': 326,\n",
       " '45,55,69,71': 327,\n",
       " '45,54,69': 328,\n",
       " '45,54,67,69': 329,\n",
       " '45,54,67': 330,\n",
       " '45,54': 331,\n",
       " '45': 332,\n",
       " '45,52,71,73': 333,\n",
       " '45,52,73': 334,\n",
       " '45,52,73,76': 335,\n",
       " '45,52,76': 336,\n",
       " '45,52': 337,\n",
       " '45,52,69': 338,\n",
       " '45,52,54,73': 339,\n",
       " '45,54,74': 340,\n",
       " '45,54,73': 341,\n",
       " '45,54,73,74': 342,\n",
       " '45,54,55,73': 343,\n",
       " '45,55': 344,\n",
       " '45,69': 345,\n",
       " '45,71': 346,\n",
       " '45,52,74': 347,\n",
       " '45,52,74,76': 348,\n",
       " '45,49,52': 349,\n",
       " '45,52,54,74': 350,\n",
       " '45,55,73': 351,\n",
       " '45,52,78': 352,\n",
       " '45,52,72,78': 353,\n",
       " '45,52,72': 354,\n",
       " '45,65': 355,\n",
       " '45,72,76': 356,\n",
       " '45,72': 357,\n",
       " '45,72,81': 358,\n",
       " '45,48,72,81': 359,\n",
       " '45,73': 360,\n",
       " '45,73,74': 361,\n",
       " '45,67,73': 362,\n",
       " '45,66': 363,\n",
       " '45,67,69': 364,\n",
       " '45,47': 365,\n",
       " '45,74': 366,\n",
       " '45,76': 367,\n",
       " '45,71,76': 368,\n",
       " '45,75,78': 369,\n",
       " '45,75': 370,\n",
       " '45,74,76': 371,\n",
       " '45,75,76': 372,\n",
       " '45,76,77': 373,\n",
       " '45,75,76,77': 374,\n",
       " '45,47,74': 375,\n",
       " '45,55,74': 376,\n",
       " '45,57,73': 377,\n",
       " '45,71,73': 378,\n",
       " '45,55,71,73': 379,\n",
       " '45,57,73,79': 380,\n",
       " '45,57,79': 381,\n",
       " '45,50,57,79': 382,\n",
       " '45,50,79': 383,\n",
       " '45,79': 384,\n",
       " '45,78': 385,\n",
       " '45,76,81': 386,\n",
       " '45,55,73,74': 387,\n",
       " '45,54,71': 388,\n",
       " '45,78,79': 389,\n",
       " '45,49': 390,\n",
       " '45,57,72': 391,\n",
       " '45,52,54': 392,\n",
       " '45,55,76': 393,\n",
       " '45,55,72': 394,\n",
       " '45,57,78': 395,\n",
       " '45,57': 396,\n",
       " '45,49,57,79': 397,\n",
       " '45,81': 398,\n",
       " '45,48,79,81': 399,\n",
       " '45,48,79': 400,\n",
       " '45,48,78,79': 401,\n",
       " '45,48,78': 402,\n",
       " '45,48': 403,\n",
       " '45,48,76': 404,\n",
       " '45,48,74,76': 405,\n",
       " '45,50,71': 406,\n",
       " '45,50,69,71': 407,\n",
       " '45,50,69': 408,\n",
       " '45,50,67,69': 409,\n",
       " '45,50,67': 410,\n",
       " '45,67': 411,\n",
       " '45,52,67,69': 412,\n",
       " '45,47,55,76': 413,\n",
       " '45,55,74,76': 414,\n",
       " '45,55,72,74': 415,\n",
       " '45,49,57,78,79': 416,\n",
       " '45,57,76,79': 417,\n",
       " '45,57,76': 418,\n",
       " '45,57,66': 419,\n",
       " '45,47,69': 420,\n",
       " '45,76,78': 421,\n",
       " '45,73,76,78': 422,\n",
       " '45,73,76': 423,\n",
       " '45,47,71,72': 424,\n",
       " '45,71,72': 425,\n",
       " '45,72,74': 426,\n",
       " '45,47,72': 427,\n",
       " '45,50,66,69': 428,\n",
       " '45,66,69': 429,\n",
       " '45,72,78': 430,\n",
       " '45,47,72,78': 431,\n",
       " '45,69,78': 432,\n",
       " '45,47,72,74': 433,\n",
       " '45,71,72,78': 434,\n",
       " '45,47,72,79': 435,\n",
       " '45,73,79': 436,\n",
       " '45,47,79': 437,\n",
       " '45,84': 438,\n",
       " '45,48,67,74': 439,\n",
       " '45,67,74': 440,\n",
       " '45,67,72': 441,\n",
       " '45,64,73': 442,\n",
       " '45,66,69,74': 443,\n",
       " '45,47,67,72': 444,\n",
       " '45,57,69': 445,\n",
       " '45,69,79': 446,\n",
       " '45,66,74': 447,\n",
       " '45,62,74': 448,\n",
       " '45,64': 449,\n",
       " '45,64,74': 450,\n",
       " '45,62,69': 451,\n",
       " '45,62,71': 452,\n",
       " '45,62': 453,\n",
       " '45,61': 454,\n",
       " '45,47,62': 455,\n",
       " '45,49,76': 456,\n",
       " '45,76,79': 457,\n",
       " '45,47,78,79': 458,\n",
       " '45,47,78': 459,\n",
       " '45,50': 460,\n",
       " '45,64,73,74': 461,\n",
       " '45,60,64': 462,\n",
       " '45,60': 463,\n",
       " '45,48,60': 464,\n",
       " '45,48,67': 465,\n",
       " '45,47,79,83': 466,\n",
       " '45,79,83': 467,\n",
       " '45,48,72': 468,\n",
       " '45,47,68': 469,\n",
       " '45,47,63': 470,\n",
       " '45,63': 471,\n",
       " '45,62,66': 472,\n",
       " '45,62,66,69': 473,\n",
       " '45,69,72': 474,\n",
       " '45,69,71,72': 475,\n",
       " '45,66,72': 476,\n",
       " '45,66,76': 477,\n",
       " '45,48,57': 478,\n",
       " '45,57,67': 479,\n",
       " '45,48,57,67': 480,\n",
       " '47,50,57,67': 481,\n",
       " '47,50,67': 482,\n",
       " '47,50,69': 483,\n",
       " '47,50,67,69': 484,\n",
       " '47,50,55,67': 485,\n",
       " '47,55,71': 486,\n",
       " '47,71': 487,\n",
       " '47,50,71': 488,\n",
       " '47,50': 489,\n",
       " '47,50,66': 490,\n",
       " '47,52': 491,\n",
       " '47,51,52,71': 492,\n",
       " '47,51,71': 493,\n",
       " '47,51,71,81': 494,\n",
       " '47,51,81': 495,\n",
       " '47,51': 496,\n",
       " '47,51,69': 497,\n",
       " '47,69': 498,\n",
       " '47': 499,\n",
       " '47,65': 500,\n",
       " '47,59,66,69,76': 501,\n",
       " '47,59,69,76': 502,\n",
       " '47,69,76': 503,\n",
       " '47,69,75': 504,\n",
       " '47,75': 505,\n",
       " '47,59,66,69': 506,\n",
       " '47,66,69,76': 507,\n",
       " '47,66,69': 508,\n",
       " '47,66,69,75': 509,\n",
       " '47,66,75': 510,\n",
       " '47,67': 511,\n",
       " '47,62': 512,\n",
       " '47,48': 513,\n",
       " '47,64': 514,\n",
       " '47,79': 515,\n",
       " '47,74': 516,\n",
       " '47,76': 517,\n",
       " '47,48,62': 518,\n",
       " '47,74,76': 519,\n",
       " '47,81': 520,\n",
       " '47,79,81': 521,\n",
       " '47,71,76': 522,\n",
       " '47,71,79': 523,\n",
       " '47,49': 524,\n",
       " '47,72,76': 525,\n",
       " '47,72': 526,\n",
       " '47,49,69,78': 527,\n",
       " '47,52,55,67': 528,\n",
       " '47,52,55,74': 529,\n",
       " '47,55,74': 530,\n",
       " '47,55': 531,\n",
       " '47,57,79': 532,\n",
       " '47,50,79': 533,\n",
       " '47,55,78': 534,\n",
       " '47,57,76': 535,\n",
       " '47,57,74,76': 536,\n",
       " '47,57,74': 537,\n",
       " '47,52,55': 538,\n",
       " '47,55,67': 539,\n",
       " '47,48,57,74': 540,\n",
       " '47,57,73,74': 541,\n",
       " '47,57,73': 542,\n",
       " '47,57,71': 543,\n",
       " '47,54,78': 544,\n",
       " '47,54,76,78': 545,\n",
       " '47,54,76': 546,\n",
       " '47,55,75': 547,\n",
       " '47,57,75': 548,\n",
       " '47,57,75,76': 549,\n",
       " '47,57': 550,\n",
       " '47,48,55': 551,\n",
       " '47,55,77': 552,\n",
       " '47,55,76,77': 553,\n",
       " '47,55,76': 554,\n",
       " '47,54,69,71': 555,\n",
       " '47,54,69': 556,\n",
       " '47,54': 557,\n",
       " '47,54,67': 558,\n",
       " '47,54,74': 559,\n",
       " '47,50,74': 560,\n",
       " '47,55,74,76': 561,\n",
       " '47,49,79': 562,\n",
       " '47,48,57,78': 563,\n",
       " '47,57,78': 564,\n",
       " '47,48,50,74': 565,\n",
       " '47,50,72,74': 566,\n",
       " '47,57,71,72': 567,\n",
       " '47,55,69': 568,\n",
       " '47,55,72': 569,\n",
       " '47,66,74': 570,\n",
       " '47,48,57,66': 571,\n",
       " '47,57,66': 572,\n",
       " '47,48,67': 573,\n",
       " '47,48,50,76': 574,\n",
       " '47,48,50,74,76': 575,\n",
       " '47,50,72': 576,\n",
       " '47,78': 577,\n",
       " '47,69,78': 578,\n",
       " '47,67,76': 579,\n",
       " '47,74,79': 580,\n",
       " '47,48,74,79': 581,\n",
       " '47,72,74': 582,\n",
       " '47,48,74': 583,\n",
       " '47,48,74,76': 584,\n",
       " '47,72,78': 585,\n",
       " '47,48,66,74': 586,\n",
       " '47,72,79': 587,\n",
       " '47,48,72,79': 588,\n",
       " '47,48,76': 589,\n",
       " '47,48,72,79,81': 590,\n",
       " '47,84': 591,\n",
       " '47,83': 592,\n",
       " '47,48,75': 593,\n",
       " '47,48,69': 594,\n",
       " '47,77,79': 595,\n",
       " '47,78,79': 596,\n",
       " '47,48,69,72,74': 597,\n",
       " '47,48,69,74': 598,\n",
       " '47,69,74': 599,\n",
       " '47,52,69,74': 600,\n",
       " '47,52,67,69,74': 601,\n",
       " '47,52,67,74': 602,\n",
       " '47,52,67': 603,\n",
       " '47,48,67,72': 604,\n",
       " '47,67,72': 605,\n",
       " '47,67,71': 606,\n",
       " '47,48,69,72': 607,\n",
       " '47,69,72,74': 608,\n",
       " '47,52,69': 609,\n",
       " '47,71,81': 610,\n",
       " '47,57,62': 611,\n",
       " '47,57,60,62': 612,\n",
       " '47,59,62,67': 613,\n",
       " '47,62,74': 614,\n",
       " '47,64,74': 615,\n",
       " '47,64,73': 616,\n",
       " '47,59': 617,\n",
       " '47,49,76,78': 618,\n",
       " '47,49,76': 619,\n",
       " '47,49,64': 620,\n",
       " '47,62,65': 621,\n",
       " '47,50,62': 622,\n",
       " '47,50,65': 623,\n",
       " '47,50,59,60': 624,\n",
       " '47,50,60': 625,\n",
       " '47,60': 626,\n",
       " '47,59,60': 627,\n",
       " '47,71,78': 628,\n",
       " '47,79,83': 629,\n",
       " '47,75,83': 630,\n",
       " '47,48,75,77': 631,\n",
       " '47,68': 632,\n",
       " '47,59,75': 633,\n",
       " '47,67,74': 634,\n",
       " '47,69,79': 635,\n",
       " '47,54,71,78': 636,\n",
       " '47,75,79': 637,\n",
       " '47,52,83': 638,\n",
       " '47,75,77': 639,\n",
       " '47,63': 640,\n",
       " '47,59,69,75': 641,\n",
       " '47,48,69,79': 642,\n",
       " '47,48,79': 643,\n",
       " '48,57,66,67': 644,\n",
       " '48,57,67': 645,\n",
       " '48,75,76': 646,\n",
       " '48,52,75,76': 647,\n",
       " '48,52,76': 648,\n",
       " '48,52,76,79': 649,\n",
       " '48,52,79': 650,\n",
       " '48,65': 651,\n",
       " '48,79': 652,\n",
       " '48,71,79': 653,\n",
       " '48,55,72,79': 654,\n",
       " '48,72,79': 655,\n",
       " '48,76,79': 656,\n",
       " '48,60,79': 657,\n",
       " '48,60,69,79': 658,\n",
       " '48,60,69': 659,\n",
       " '48,59,76': 660,\n",
       " '48,76': 661,\n",
       " '48,72,81': 662,\n",
       " '48,72,79,81': 663,\n",
       " '48,50,72,79': 664,\n",
       " '48,67,75': 665,\n",
       " '48,75': 666,\n",
       " '48,55,64,71': 667,\n",
       " '48,63,65': 668,\n",
       " '48,59,79': 669,\n",
       " '48,69,79': 670,\n",
       " '48': 671,\n",
       " '48,55': 672,\n",
       " '48,55,64': 673,\n",
       " '48,69': 674,\n",
       " '48,64': 675,\n",
       " '48,62': 676,\n",
       " '48,74': 677,\n",
       " '48,50,64': 678,\n",
       " '48,49,67': 679,\n",
       " '48,49': 680,\n",
       " '48,79,81': 681,\n",
       " '48,81': 682,\n",
       " '48,55,74': 683,\n",
       " '48,55,76': 684,\n",
       " '48,57,76': 685,\n",
       " '48,78': 686,\n",
       " '48,57,78': 687,\n",
       " '48,57,79': 688,\n",
       " '48,57,78,79': 689,\n",
       " '48,57': 690,\n",
       " '48,57,76,78': 691,\n",
       " '48,57,74,76': 692,\n",
       " '48,57,74': 693,\n",
       " '48,52,55,72': 694,\n",
       " '48,55,72': 695,\n",
       " '48,55,72,74': 696,\n",
       " '48,55,74,76': 697,\n",
       " '48,62,78': 698,\n",
       " '48,57,81': 699,\n",
       " '48,57,79,81': 700,\n",
       " '48,60,76': 701,\n",
       " '48,57,71': 702,\n",
       " '48,57,69,71': 703,\n",
       " '48,57,69': 704,\n",
       " '48,50,55,72': 705,\n",
       " '48,50,55,71,72': 706,\n",
       " '48,55,71,72': 707,\n",
       " '48,55,71': 708,\n",
       " '48,55,69': 709,\n",
       " '48,55,69,71': 710,\n",
       " '48,55,67,69': 711,\n",
       " '48,55,67': 712,\n",
       " '48,55,77': 713,\n",
       " '48,55,76,77': 714,\n",
       " '48,52,55,76,77': 715,\n",
       " '48,52,55,76': 716,\n",
       " '48,53,76': 717,\n",
       " '48,60,72': 718,\n",
       " '48,60': 719,\n",
       " '48,54': 720,\n",
       " '48,54,71': 721,\n",
       " '48,54,69,71': 722,\n",
       " '48,52,67': 723,\n",
       " '48,52': 724,\n",
       " '48,52,66': 725,\n",
       " '48,54,62': 726,\n",
       " '48,50,57': 727,\n",
       " '48,50,74': 728,\n",
       " '48,52,57,69': 729,\n",
       " '48,72': 730,\n",
       " '48,52,69': 731,\n",
       " '48,50,57,78': 732,\n",
       " '48,66': 733,\n",
       " '48,57,66': 734,\n",
       " '48,64,66': 735,\n",
       " '48,76,78': 736,\n",
       " '48,55,71,76': 737,\n",
       " '48,67,76': 738,\n",
       " '48,67': 739,\n",
       " '48,74,79': 740,\n",
       " '48,66,74': 741,\n",
       " '48,50,66,74': 742,\n",
       " '48,50,66': 743,\n",
       " '48,50,72,81': 744,\n",
       " '48,50,74,79': 745,\n",
       " '48,69,72': 746,\n",
       " '48,50,78': 747,\n",
       " '48,50': 748,\n",
       " '48,50,71': 749,\n",
       " '48,84': 750,\n",
       " '48,76,77': 751,\n",
       " '48,72,74': 752,\n",
       " '48,52,67,76': 753,\n",
       " '48,67,74,76': 754,\n",
       " '48,67,72': 755,\n",
       " '48,67,74': 756,\n",
       " '48,67,72,74': 757,\n",
       " '48,69,71,72': 758,\n",
       " '48,67,79': 759,\n",
       " '48,67,76,79': 760,\n",
       " '48,66,76': 761,\n",
       " '48,57,62': 762,\n",
       " '48,59,64': 763,\n",
       " '48,55,59,64': 764,\n",
       " '48,52,64': 765,\n",
       " '48,76,83': 766,\n",
       " '48,80': 767,\n",
       " '48,77,81': 768,\n",
       " '48,77': 769,\n",
       " '48,63': 770,\n",
       " '48,51,72': 771,\n",
       " '48,69,76': 772,\n",
       " '48,69,72,78': 773,\n",
       " '48,69,78': 774,\n",
       " '48,55,64,67': 775,\n",
       " '48,64,67': 776,\n",
       " '48,55,57': 777,\n",
       " '48,68': 778,\n",
       " '48,63,64': 779,\n",
       " '48,50,69': 780,\n",
       " '48,50,72': 781,\n",
       " '48,50,64,67': 782,\n",
       " '49': 783,\n",
       " '49,64': 784,\n",
       " '49,64,76': 785,\n",
       " '49,76': 786,\n",
       " '49,52,69': 787,\n",
       " '49,69': 788,\n",
       " '49,57,76': 789,\n",
       " '49,57,67,76': 790,\n",
       " '49,57,67': 791,\n",
       " '49,57': 792,\n",
       " '49,69,79': 793,\n",
       " '49,50,69,78,79': 794,\n",
       " '49,71': 795,\n",
       " '49,69,78,79': 796,\n",
       " '49,50,78': 797,\n",
       " '49,51,69': 798,\n",
       " '49,79': 799,\n",
       " '49,67': 800,\n",
       " '49,76,79': 801,\n",
       " '49,83': 802,\n",
       " '49,81': 803,\n",
       " '49,78': 804,\n",
       " '49,69,78': 805,\n",
       " '49,51,78': 806,\n",
       " '49,55,71,76': 807,\n",
       " '49,55,76': 808,\n",
       " '49,52,67,76': 809,\n",
       " '49,52,76': 810,\n",
       " '49,78,81': 811,\n",
       " '49,50,76,78': 812,\n",
       " '49,76,78': 813,\n",
       " '49,52,79': 814,\n",
       " '49,52,79,81': 815,\n",
       " '49,52,81': 816,\n",
       " '49,81,83': 817,\n",
       " '49,57,81,83': 818,\n",
       " '49,57,81': 819,\n",
       " '49,57,78': 820,\n",
       " '49,57,83': 821,\n",
       " '49,57,78,81': 822,\n",
       " '49,76,83': 823,\n",
       " '49,75': 824,\n",
       " '49,70': 825,\n",
       " '49,50,79': 826,\n",
       " '49,50,76,79': 827,\n",
       " '49,54': 828,\n",
       " '49,69,76': 829,\n",
       " '49,50': 830,\n",
       " '49,50,74': 831,\n",
       " '49,66': 832,\n",
       " '49,50,66': 833,\n",
       " '49,50,64,66': 834,\n",
       " '49,50,52,69': 835,\n",
       " '49,52,69,71': 836,\n",
       " '49,52,71': 837,\n",
       " '49,52,73': 838,\n",
       " '49,52,73,76': 839,\n",
       " '49,64,66': 840,\n",
       " '49,50,52,71': 841,\n",
       " '49,50,52,69,71': 842,\n",
       " '49,52': 843,\n",
       " '49,52,74': 844,\n",
       " '49,74': 845,\n",
       " '49,74,76': 846,\n",
       " '49,57,69': 847,\n",
       " '49,50,57,69': 848,\n",
       " '49,79,81': 849,\n",
       " '49,76,79,81': 850,\n",
       " '49,76,81': 851,\n",
       " '50,71': 852,\n",
       " '50,57,67': 853,\n",
       " '50,57,66': 854,\n",
       " '50,57,64,66': 855,\n",
       " '50,54,57,64,66': 856,\n",
       " '50,54,64,66': 857,\n",
       " '50,54,64': 858,\n",
       " '50,64': 859,\n",
       " '50,62': 860,\n",
       " '50,62,64': 861,\n",
       " '50,55,67': 862,\n",
       " '50,55,67,69': 863,\n",
       " '50,55,69': 864,\n",
       " '50,55': 865,\n",
       " '50,55,71': 866,\n",
       " '50,62,78': 867,\n",
       " '50,62,78,81': 868,\n",
       " '50,62,81': 869,\n",
       " '50,62,71,72': 870,\n",
       " '50,62,71': 871,\n",
       " '50,69,78,79': 872,\n",
       " '50,69,78': 873,\n",
       " '50,78': 874,\n",
       " '50,76,78': 875,\n",
       " '50,76': 876,\n",
       " '50,74,78': 877,\n",
       " '50,54,74': 878,\n",
       " '50,54,62,74': 879,\n",
       " '50,62,74': 880,\n",
       " '50,54,78,81': 881,\n",
       " '50,54,81': 882,\n",
       " '50,57,62,66,69,74': 883,\n",
       " '50,57,62,66,74': 884,\n",
       " '50,57,62,66': 885,\n",
       " '50,57,62': 886,\n",
       " '50,66,71': 887,\n",
       " '50,57,76': 888,\n",
       " '50,57,67,76': 889,\n",
       " '50,54,57,66': 890,\n",
       " '50,54,62': 891,\n",
       " '50,57,62,64': 892,\n",
       " '50,62,76,78': 893,\n",
       " '50,54': 894,\n",
       " '50,54,62,79': 895,\n",
       " '50,54,79': 896,\n",
       " '50,57,62,69,74': 897,\n",
       " '50,57,62,69': 898,\n",
       " '50,57': 899,\n",
       " '50': 900,\n",
       " '50,69,74,78': 901,\n",
       " '50,54,69,74,78': 902,\n",
       " '50,54,72': 903,\n",
       " '50,72': 904,\n",
       " '50,69': 905,\n",
       " '50,67,69': 906,\n",
       " '50,67': 907,\n",
       " '50,54,62,72': 908,\n",
       " '50,62,72': 909,\n",
       " '50,72,79': 910,\n",
       " '50,72,76': 911,\n",
       " '50,66': 912,\n",
       " '50,62,66': 913,\n",
       " '50,54,62,69': 914,\n",
       " '50,59,62,69': 915,\n",
       " '50,59,62': 916,\n",
       " '50,59,62,67': 917,\n",
       " '50,59,67': 918,\n",
       " '50,57,59,60,67': 919,\n",
       " '50,57,60,67': 920,\n",
       " '50,57,60': 921,\n",
       " '50,57,60,66': 922,\n",
       " '50,55,59,62,67': 923,\n",
       " '50,69,74': 924,\n",
       " '50,54,78': 925,\n",
       " '50,52,69': 926,\n",
       " '50,72,79,81': 927,\n",
       " '50,54,66': 928,\n",
       " '50,54,62,66': 929,\n",
       " '50,59,60,67': 930,\n",
       " '50,55,62': 931,\n",
       " '50,71,72': 932,\n",
       " '50,52': 933,\n",
       " '50,73': 934,\n",
       " '50,54,73': 935,\n",
       " '50,71,73': 936,\n",
       " '50,54,69': 937,\n",
       " '50,79': 938,\n",
       " '50,78,79': 939,\n",
       " '50,62,79': 940,\n",
       " '50,74': 941,\n",
       " '50,81': 942,\n",
       " '50,62,78,79': 943,\n",
       " '50,60': 944,\n",
       " '50,60,69': 945,\n",
       " '50,52,55,69': 946,\n",
       " '50,57,69': 947,\n",
       " '50,62,69': 948,\n",
       " '50,54,55,69,74': 949,\n",
       " '50,54,69,74': 950,\n",
       " '50,52,54': 951,\n",
       " '50,52,72': 952,\n",
       " '50,52,71': 953,\n",
       " '50,54,71': 954,\n",
       " '50,54,71,72': 955,\n",
       " '50,54,55': 956,\n",
       " '50,54,55,72': 957,\n",
       " '50,55,72': 958,\n",
       " '50,55,73,74': 959,\n",
       " '50,55,73': 960,\n",
       " '50,52,67': 961,\n",
       " '50,55,74': 962,\n",
       " '50,54,55,74': 963,\n",
       " '50,52,54,74': 964,\n",
       " '50,52,74': 965,\n",
       " '50,55,69,74': 966,\n",
       " '50,52,54,71': 967,\n",
       " '50,60,76': 968,\n",
       " '50,59,60,74,76': 969,\n",
       " '50,59,74,76': 970,\n",
       " '50,59,74': 971,\n",
       " '50,52,55,71': 972,\n",
       " '50,55,71,72': 973,\n",
       " '50,69,71': 974,\n",
       " '50,59': 975,\n",
       " '50,57,59,67': 976,\n",
       " '50,55,66': 977,\n",
       " '50,55,64': 978,\n",
       " '50,54,55,62': 979,\n",
       " '50,57,78': 980,\n",
       " '50,60,76,78': 981,\n",
       " '50,57,74': 982,\n",
       " '50,52,55,72': 983,\n",
       " '50,52,55': 984,\n",
       " '50,54,55,71': 985,\n",
       " '50,54,60,69': 986,\n",
       " '50,60,67,69': 987,\n",
       " '50,60,67': 988,\n",
       " '50,55,60,67': 989,\n",
       " '50,59,76': 990,\n",
       " '50,55,64,66': 991,\n",
       " '50,57,79': 992,\n",
       " '50,57,78,79': 993,\n",
       " '50,52,54,55': 994,\n",
       " '50,74,79': 995,\n",
       " '50,52,74,79': 996,\n",
       " '50,66,69': 997,\n",
       " '50,66,74': 998,\n",
       " '50,72,81': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13120,\n",
       " 17124,\n",
       " 36315,\n",
       " 3080,\n",
       " 24711,\n",
       " 12182,\n",
       " 35070,\n",
       " 28910,\n",
       " 32797,\n",
       " 43508,\n",
       " 10734,\n",
       " 39948,\n",
       " 27684,\n",
       " 5700,\n",
       " 1074,\n",
       " 17678,\n",
       " 28609,\n",
       " 19528,\n",
       " 40866,\n",
       " 37212,\n",
       " 35018,\n",
       " 19109,\n",
       " 3373,\n",
       " 35822,\n",
       " 728,\n",
       " 4124,\n",
       " 24160,\n",
       " 17875,\n",
       " 6451,\n",
       " 20678,\n",
       " 7826,\n",
       " 15269,\n",
       " 9827,\n",
       " 3883,\n",
       " 15263,\n",
       " 19558,\n",
       " 3537,\n",
       " 1121,\n",
       " 39937,\n",
       " 20074,\n",
       " 33437,\n",
       " 8989,\n",
       " 10626,\n",
       " 28489,\n",
       " 14599,\n",
       " 5020,\n",
       " 36873,\n",
       " 22882,\n",
       " 19777,\n",
       " 40001,\n",
       " 273,\n",
       " 11327,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 1107,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 1368,\n",
       " 2033,\n",
       " 1154,\n",
       " 3443,\n",
       " 3443,\n",
       " 2576,\n",
       " 2612,\n",
       " 1826,\n",
       " 904,\n",
       " 2076,\n",
       " 1877,\n",
       " 1877,\n",
       " 1877,\n",
       " 1877,\n",
       " 1877,\n",
       " 1877,\n",
       " 1877,\n",
       " 1877,\n",
       " 1877,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2150,\n",
       " 2150,\n",
       " 2150,\n",
       " 2150,\n",
       " 2150,\n",
       " 2150,\n",
       " 2150,\n",
       " 2150,\n",
       " 2150,\n",
       " 2150,\n",
       " 1975,\n",
       " 1975,\n",
       " 1975,\n",
       " 1975,\n",
       " 1975,\n",
       " 1975,\n",
       " 1975,\n",
       " 1975,\n",
       " 1975,\n",
       " 1975,\n",
       " 1975,\n",
       " 2151,\n",
       " 1975,\n",
       " 6565,\n",
       " 1276,\n",
       " 1600,\n",
       " 1600,\n",
       " 1861,\n",
       " 1954,\n",
       " 1861,\n",
       " 2394,\n",
       " 1954,\n",
       " 1954,\n",
       " 1954,\n",
       " 1954,\n",
       " 6261,\n",
       " 1947,\n",
       " 1947,\n",
       " 22406,\n",
       " 2729,\n",
       " 40793,\n",
       " 12941,\n",
       " 331,\n",
       " 331,\n",
       " 10440,\n",
       " 1724,\n",
       " 9733,\n",
       " 41601,\n",
       " 9748,\n",
       " 23979,\n",
       " 33171,\n",
       " 33171,\n",
       " 33171,\n",
       " 33171,\n",
       " 33171,\n",
       " 1958,\n",
       " 10532,\n",
       " 6168,\n",
       " 15628,\n",
       " 21057,\n",
       " 12470,\n",
       " 6090,\n",
       " 40664,\n",
       " 18708,\n",
       " 13102,\n",
       " 6245,\n",
       " 1943,\n",
       " 1943,\n",
       " 14709,\n",
       " 1944,\n",
       " 2129,\n",
       " 2129,\n",
       " 2129,\n",
       " 2129,\n",
       " 2129,\n",
       " 2129,\n",
       " 2129,\n",
       " 2129,\n",
       " 2129,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2164,\n",
       " 4260,\n",
       " 6316,\n",
       " 2027,\n",
       " 2027,\n",
       " 1564,\n",
       " 2027,\n",
       " 2027,\n",
       " 1530,\n",
       " 1530,\n",
       " 1530,\n",
       " 1530,\n",
       " 14825,\n",
       " 2027,\n",
       " 2148,\n",
       " 1297,\n",
       " 37232,\n",
       " 22629,\n",
       " 18632,\n",
       " 39776,\n",
       " 25046,\n",
       " 5933,\n",
       " 40219,\n",
       " 40298,\n",
       " 40298,\n",
       " 208,\n",
       " 208,\n",
       " 208,\n",
       " 1923,\n",
       " 14629,\n",
       " 1554,\n",
       " 1554]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '38,76', '40,67,74', '43,74', '43,74', '45,76,81', '47,67', '47,67', '47,67', '47,67', '48,81', '49,57,67,76', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55', '55,71', '56,71,72', '56', '56', '56', '56', '56', '57', '57', '57', '57', '57', '57', '57', '57,67,76', '57,67,76', '57,67,76', '57,66,81', '57,65,69,74', '62', '62', '62,76', '62,66,76', '62,66,76', '62,70', '62,69,81', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '64', '65', '65', '65', '66,84', '69,78,81', '70', '70', '70', '70', '70', '70', '70', '70', '70', '70', '76,77', '76,79', '76,79', '79', '79', '79', '81', '61,77', '63,75,80', '41,48,57,69', '53,65,69,72', '58,63', '69,81,88', '74,77,81', '44,59', '45,48,84,91', '51,77', '62,86', '65,83,85,86', '69,73,76', '37,54', '40,76,88', '42,54,61', '45,68,73', '50,58,70', '52,64,69,73', '52,60,71,79', '54,58,63', '58,68,71,83,87', '58,65,70,72,77', '60,81,86', '62,68,71,77,83', '37,44,49,58,61', '41,55,63', '61,91', '46,63,72', '47,54,59,63', '62,65,66,71', '51,53,56,62,80', '45,61,67,70', '79,84,87', '42,49,57,64,66', '46,49,72,73', '28,68,73', '33,71,73,80', '44,68,77,80', '45,54,60,66,69,72', '57,61,77,78,85', '57,61,77,85', '30,59,67', '30,42,51,56,60', '56,63,73,75', '28,56,59', '56,58,62,67,72', '33,38,50,51,53,54', '33,45,84', '45,93,95', '46,50,53,74,77,82,86', '48,60,66', '52,55,58,60,79', '33,37,40,43,69', '48,58,72,76', '69,70,74,81', '50,56,61', '59,66,71,90,95,96,98,102']\n",
      "Tempo 208.6092715231787\n"
     ]
    }
   ],
   "source": [
    "write_midi_file_from_generated(generate,dict_index, \"random.mid\", start_index=seq_len-1, fs=7, max_generated = max_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
