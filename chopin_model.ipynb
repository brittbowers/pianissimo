{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy # used for integer targets\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from random import shuffle, seed, sample\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding all titles, training, and target sets composed by Chopin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Input file into title, train, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [01:21,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "train = []\n",
    "target = []\n",
    "with open('data/Chopin_input.json', 'r') as handle: # EDIT WITH COMPOSER\n",
    "    for i,line in enumerate(tqdm(handle)):\n",
    "        song = json.loads(line)\n",
    "        titles.append(song['title'])\n",
    "        train.append(song['train'])\n",
    "        target.append(song['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below for Parsing Chopin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chopin = []\n",
    "keep_indices = []\n",
    "for i,song in enumerate(titles):\n",
    "    song = song.lower()\n",
    "    if 'andante' in song:\n",
    "        song = 'andante op 22'\n",
    "    if 'ballade no. 2' in song:\n",
    "        song = 'ballade no 2'\n",
    "    if 'barcarolle' in song:\n",
    "        song = 'barcarolle'\n",
    "    if 'berceuse' in song:\n",
    "        song = 'barceuse'\n",
    "    if 'etude op. 10 no. 1' in song:\n",
    "        song = 'etude op. 10 no. 1'\n",
    "    if 'etude op. 10 no. 12' in song:\n",
    "        song = 'etude op. 10 no. 12'\n",
    "    if 'etude op. 10 no. 2' in song:\n",
    "        song = 'etude op. 10 no. 2'\n",
    "    if 'etude op. 10 no. 4' in song:\n",
    "        song = 'etude op. 10 no. 4'\n",
    "    if 'etude op. 10 no. 8' in song:\n",
    "        song = 'etude op. 10 no. 8'\n",
    "    if 'etude op. 25 no. 1' in song:\n",
    "        song = 'etude op. 25 no. 1'\n",
    "    if 'etude op. 25 no. 10' in song:\n",
    "        song = 'etude op. 25 no. 10'\n",
    "    if 'etude op. 25 no. 11' in song:\n",
    "        song = 'etude op. 25 no. 11'\n",
    "    if 'etude op. 25 no. 12' in song:\n",
    "        song = 'etude op. 25 no. 12'\n",
    "    if 'etude op. 25 no. 6' in song:\n",
    "        song = 'etude op. 25 no. 6'\n",
    "    if 'op. 49' in song:\n",
    "        song = 'fantasy op. 49'\n",
    "    if 'polonaise-fantasie' in song:\n",
    "        song = 'polonaise-fantasie'\n",
    "    if 'scherzo no. 2' in song:\n",
    "        song = 'scherzo no. 2'\n",
    "    if 'scherzo no. 3' in song:\n",
    "        song = 'scherzo no. 3'\n",
    "    if 'scherzo no. 4' in song:\n",
    "        song = 'scherzo no. 4'\n",
    "    if 'sonata no. 3' in song:\n",
    "        song = 'sonata no. 3'\n",
    "    if song not in unique_chopin:\n",
    "        unique_chopin.append(song)\n",
    "        keep_indices.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below for Parsing Brahms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_brahms = []\n",
    "keep_indices = []\n",
    "for i,song in enumerate(titles):\n",
    "    song = song.lower()\n",
    "    if 'sonata in' in song:\n",
    "        song = 'sonata in f minor op 5'\n",
    "    if song not in unique_brahms:\n",
    "        unique_brahms.append(song)\n",
    "        keep_indices.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping Unique Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_train = []\n",
    "for i,each in enumerate(train):\n",
    "    if i in keep_indices:\n",
    "        unique_train.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_target = []\n",
    "for i, each in enumerate(target):\n",
    "    if i in keep_indices:\n",
    "        unique_target.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_chopin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_len, unique_notes, dropout=0.3, output_emb=100, rnn_unit=128, dense_unit=64):\n",
    "    inputs = tf.keras.layers.Input(shape=(seq_len,))\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=unique_notes, output_dim=output_emb, input_length=seq_len)(inputs)\n",
    "    forward_pass = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_unit, return_sequences=True))(embedding)\n",
    "#     #forward_pass , att_vector = SeqSelfAttention(\n",
    "#         return_attention=True,\n",
    "#         attention_activation='sigmoid',\n",
    "#         attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "#         attention_width=50,\n",
    "#         kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "#         bias_regularizer=tf.keras.regularizers.l1(1e-4),\n",
    "#         attention_regularizer_weight=1e-4,\n",
    "#         )(forward_pass)\n",
    "    forward_pass = tf.keras.layers.Dropout(dropout)(forward_pass)\n",
    "    forward_pass = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_unit, return_sequences=True))(forward_pass)\n",
    "#     forward_pass , att_vector2 = SeqSelfAttention(\n",
    "#         return_attention=True,\n",
    "#         attention_activation='sigmoid',\n",
    "#         attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, # multiplicative\n",
    "#         attention_width=50,\n",
    "#         kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "#         bias_regularizer=tf.keras.regularizers.l1(1e-4),\n",
    "#         attention_regularizer_weight=1e-4,\n",
    "#         )(forward_pass)\n",
    "    forward_pass = tf.keras.layers.Dropout(dropout)(forward_pass)\n",
    "    forward_pass = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_unit))(forward_pass)\n",
    "    forward_pass = tf.keras.layers.Dropout(dropout)(forward_pass)\n",
    "    forward_pass = tf.keras.layers.Dense(dense_unit)(forward_pass)\n",
    "    forward_pass = tf.keras.layers.LeakyReLU()(forward_pass)\n",
    "    outputs = tf.keras.layers.Dense(unique_notes, activation = \"softmax\")(forward_pass)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='generate_scores_rnn')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Chopin_index.json', 'r') as read_file:\n",
    "    dict_index = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_notes = len(dict_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(50, unique_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generate_scores_rnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 50, 100)           11209000  \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 50, 256)           175872    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 50, 256)           295680    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, 256)               295680    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 112090)            7285850   \n",
      "=================================================================\n",
      "Total params: 19,278,530\n",
      "Trainable params: 19,278,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Nadam()\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 model=model)\n",
    "checkpoint_dir = 'models/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "loss_fn = sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rnn_models\n",
    "# from rnn_models import SeqSelfAttention\n",
    "# from rnn_models import TrainModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Class for self sequecing layer from github\n",
    "class SeqSelfAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    ATTENTION_TYPE_ADD = 'additive'\n",
    "    ATTENTION_TYPE_MUL = 'multiplicative'\n",
    "\n",
    "    def __init__(self,\n",
    "                 units=32,\n",
    "                 attention_width=None,\n",
    "                 attention_type=ATTENTION_TYPE_ADD,\n",
    "                 return_attention=False,\n",
    "                 history_only=False,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 use_additive_bias=True,\n",
    "                 use_attention_bias=True,\n",
    "                 attention_activation=None,\n",
    "                 attention_regularizer_weight=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Layer initialization.\n",
    "        For additive attention, see: https://arxiv.org/pdf/1806.01264.pdf\n",
    "        :param units: The dimension of the vectors that used to calculate the attention weights.\n",
    "        :param attention_width: The width of local attention.\n",
    "        :param attention_type: 'additive' or 'multiplicative'.\n",
    "        :param return_attention: Whether to return the attention weights for visualization.\n",
    "        :param history_only: Only use historical pieces of data.\n",
    "        :param kernel_initializer: The initializer for weight matrices.\n",
    "        :param bias_initializer: The initializer for biases.\n",
    "        :param kernel_regularizer: The regularization for weight matrices.\n",
    "        :param bias_regularizer: The regularization for biases.\n",
    "        :param kernel_constraint: The constraint for weight matrices.\n",
    "        :param bias_constraint: The constraint for biases.\n",
    "        :param use_additive_bias: Whether to use bias while calculating the relevance of inputs features\n",
    "                                  in additive mode.\n",
    "        :param use_attention_bias: Whether to use bias while calculating the weights of attention.\n",
    "        :param attention_activation: The activation used for calculating the weights of attention.\n",
    "        :param attention_regularizer_weight: The weights of attention regularizer.\n",
    "        :param kwargs: Parameters for parent class.\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.units = units\n",
    "        self.attention_width = attention_width\n",
    "        self.attention_type = attention_type\n",
    "        self.return_attention = return_attention\n",
    "        self.history_only = history_only\n",
    "        if history_only and attention_width is None:\n",
    "            self.attention_width = int(1e9)\n",
    "\n",
    "        self.use_additive_bias = use_additive_bias\n",
    "        self.use_attention_bias = use_attention_bias\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n",
    "        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n",
    "        self.attention_activation = tf.keras.activations.get(attention_activation)\n",
    "        self.attention_regularizer_weight = attention_regularizer_weight\n",
    "        self._backend = tf.keras.backend.backend()\n",
    "\n",
    "        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
    "            self.Wx, self.Wt, self.bh = None, None, None\n",
    "            self.Wa, self.ba = None, None\n",
    "        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
    "            self.Wa, self.ba = None, None\n",
    "        else:\n",
    "            raise NotImplementedError('No implementation for attention type : ' + attention_type)\n",
    "\n",
    "        super(SeqSelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'attention_width': self.attention_width,\n",
    "            'attention_type': self.attention_type,\n",
    "            'return_attention': self.return_attention,\n",
    "            'history_only': self.history_only,\n",
    "            'use_additive_bias': self.use_additive_bias,\n",
    "            'use_attention_bias': self.use_attention_bias,\n",
    "            'kernel_initializer': tf.keras.regularizers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': tf.keras.regularizers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': tf.keras.regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': tf.keras.regularizers.serialize(self.bias_regularizer),\n",
    "            'kernel_constraint': tf.keras.constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': tf.keras.constraints.serialize(self.bias_constraint),\n",
    "            'attention_activation': tf.keras.activations.serialize(self.attention_activation),\n",
    "            'attention_regularizer_weight': self.attention_regularizer_weight,\n",
    "        }\n",
    "        base_config = super(SeqSelfAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape = input_shape[0]\n",
    "        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
    "            self._build_additive_attention(input_shape)\n",
    "        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
    "            self._build_multiplicative_attention(input_shape)\n",
    "        super(SeqSelfAttention, self).build(input_shape)\n",
    "\n",
    "    def _build_additive_attention(self, input_shape):\n",
    "        feature_dim = input_shape[2]\n",
    "\n",
    "        self.Wt = self.add_weight(shape=(feature_dim, self.units),\n",
    "                                  name='{}_Add_Wt'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        self.Wx = self.add_weight(shape=(feature_dim, self.units),\n",
    "                                  name='{}_Add_Wx'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        if self.use_additive_bias:\n",
    "            self.bh = self.add_weight(shape=(self.units,),\n",
    "                                      name='{}_Add_bh'.format(self.name),\n",
    "                                      initializer=self.bias_initializer,\n",
    "                                      regularizer=self.bias_regularizer,\n",
    "                                      constraint=self.bias_constraint)\n",
    "\n",
    "        self.Wa = self.add_weight(shape=(self.units, 1),\n",
    "                                  name='{}_Add_Wa'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        if self.use_attention_bias:\n",
    "            self.ba = self.add_weight(shape=(1,),\n",
    "                                      name='{}_Add_ba'.format(self.name),\n",
    "                                      initializer=self.bias_initializer,\n",
    "                                      regularizer=self.bias_regularizer,\n",
    "                                      constraint=self.bias_constraint)\n",
    "\n",
    "    def _build_multiplicative_attention(self, input_shape):\n",
    "        feature_dim = input_shape[2]\n",
    "\n",
    "        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),\n",
    "                                  name='{}_Mul_Wa'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        if self.use_attention_bias:\n",
    "            self.ba = self.add_weight(shape=(1,),\n",
    "                                      name='{}_Mul_ba'.format(self.name),\n",
    "                                      initializer=self.bias_initializer,\n",
    "                                      regularizer=self.bias_regularizer,\n",
    "                                      constraint=self.bias_constraint)\n",
    "\n",
    "    def call(self, inputs, mask=None, **kwargs):\n",
    "        if isinstance(inputs, list):\n",
    "            inputs, positions = inputs\n",
    "            positions = K.cast(positions, 'int32')\n",
    "            mask = mask[1]\n",
    "        else:\n",
    "            positions = None\n",
    "\n",
    "        input_len = K.shape(inputs)[1]\n",
    "\n",
    "        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
    "            e = self._call_additive_emission(inputs)\n",
    "        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
    "            e = self._call_multiplicative_emission(inputs)\n",
    "\n",
    "        if self.attention_activation is not None:\n",
    "            e = self.attention_activation(e)\n",
    "        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n",
    "        if self.attention_width is not None:\n",
    "            ones = tf.ones((input_len, input_len))\n",
    "            if self.history_only:\n",
    "                local = tf.linalg.band_part(\n",
    "                    ones,\n",
    "                    K.minimum(input_len, self.attention_width - 1),\n",
    "                    0,\n",
    "                )\n",
    "            else:\n",
    "                local = tf.linalg.band_part(\n",
    "                    ones,\n",
    "                    K.minimum(input_len, self.attention_width // 2),\n",
    "                    K.minimum(input_len, (self.attention_width - 1) // 2),\n",
    "                )\n",
    "            e = e * K.expand_dims(local, 0)\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask)\n",
    "            e = K.permute_dimensions(K.permute_dimensions(e * mask, (0, 2, 1)) * mask, (0, 2, 1))\n",
    "\n",
    "        # a_{t} = \\text{softmax}(e_t)\n",
    "        s = K.sum(e, axis=-1)\n",
    "        s = K.tile(K.expand_dims(s, axis=-1), K.stack([1, 1, input_len]))\n",
    "        a = e / (s + K.epsilon())\n",
    "\n",
    "        # l_t = \\sum_{t'} a_{t, t'} x_{t'}\n",
    "        v = K.batch_dot(a, inputs)\n",
    "        if self.attention_regularizer_weight > 0.0:\n",
    "            self.add_loss(self._attention_regularizer(a))\n",
    "\n",
    "        if positions is not None:\n",
    "            pos_num = K.shape(positions)[1]\n",
    "            batch_indices = K.tile(K.expand_dims(K.arange(K.shape(inputs)[0]), axis=-1), K.stack([1, pos_num]))\n",
    "            pos_indices = K.stack([batch_indices, positions], axis=-1)\n",
    "            v = tf.gather_nd(v, pos_indices)\n",
    "            a = tf.gather_nd(a, pos_indices)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [v, a]\n",
    "        return v\n",
    "\n",
    "    def _call_additive_emission(self, inputs):\n",
    "        input_shape = K.shape(inputs)\n",
    "        batch_size, input_len = input_shape[0], input_shape[1]\n",
    "\n",
    "        # h_{t, t'} = \\tanh(x_t^T W_t + x_{t'}^T W_x + b_h)\n",
    "        q, k = K.dot(inputs, self.Wt), K.dot(inputs, self.Wx)\n",
    "        q = K.tile(K.expand_dims(q, 2), K.stack([1, 1, input_len, 1]))\n",
    "        k = K.tile(K.expand_dims(k, 1), K.stack([1, input_len, 1, 1]))\n",
    "        if self.use_additive_bias:\n",
    "            h = K.tanh(q + k + self.bh)\n",
    "        else:\n",
    "            h = K.tanh(q + k)\n",
    "\n",
    "        # e_{t, t'} = W_a h_{t, t'} + b_a\n",
    "        if self.use_attention_bias:\n",
    "            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))\n",
    "        else:\n",
    "            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))\n",
    "        return e\n",
    "\n",
    "    def _call_multiplicative_emission(self, inputs):\n",
    "        # e_{t, t'} = x_t^T W_a x_{t'} + b_a\n",
    "        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))\n",
    "        if self.use_attention_bias:\n",
    "            e = e + self.ba\n",
    "        return e\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape, pos_shape = input_shape\n",
    "            output_shape = (input_shape[0], pos_shape[1], input_shape[2])\n",
    "        else:\n",
    "            output_shape = input_shape\n",
    "        if self.return_attention:\n",
    "            attention_shape = (input_shape[0], output_shape[1], input_shape[1])\n",
    "            return [output_shape, attention_shape]\n",
    "        return output_shape\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if isinstance(inputs, list):\n",
    "            mask = mask[1]\n",
    "        if self.return_attention:\n",
    "            return [mask, None]\n",
    "        return mask\n",
    "\n",
    "    def _attention_regularizer(self, attention):\n",
    "        batch_size = K.cast(K.shape(attention)[0], K.floatx())\n",
    "        input_len = K.shape(attention)[-1]\n",
    "        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(\n",
    "            attention,\n",
    "            K.permute_dimensions(attention, (0, 2, 1))) - tf.eye(input_len))) / batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def get_custom_objects():\n",
    "        return {'SeqSelfAttention': SeqSelfAttention}\n",
    "\n",
    "# Training Model Class\n",
    "class TrainModel:\n",
    "\n",
    "    def __init__(self, epochs, sample_train, sample_target, batch_nnet_size, batch_song, optimizer, checkpoint, loss_fn,checkpoint_prefix, total_songs, model):\n",
    "        self.epochs = epochs\n",
    "        self.sample_train = sample_train\n",
    "        self.sample_target = sample_target\n",
    "        self.batch_nnet_size = batch_nnet_size\n",
    "        self.batch_song = batch_song\n",
    "        self.optimizer = optimizer\n",
    "        self.checkpoint = checkpoint\n",
    "        self.loss_fn = loss_fn\n",
    "        self.checkpoint_prefix = checkpoint_prefix\n",
    "        self.total_songs = total_songs\n",
    "        self.model = model\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in tqdm_notebook(range(self.epochs),desc='epochs'):\n",
    "            # for each epoch, shuffle the list of all the songs\n",
    "            c = list(zip(self.sample_train, self.sample_target))\n",
    "            shuffle(c)\n",
    "            self.sample_train, self.sample_target = zip(*c)\n",
    "            loss_total = 0\n",
    "            steps = 0\n",
    "            steps_nnet = 0\n",
    "            # Iterate all songs by the length of sample input (total_songs) and batches (batch_song)\n",
    "            for i in tqdm_notebook(range(0,self.total_songs, self.batch_song), desc='MUSIC'):\n",
    "                # EXAMPLE: [0,5,10,15,20] FOR TOTAL_SONGS = 20 AND BATCH_SONG = 5\n",
    "                steps += 1\n",
    "                #inputs_nnet_large, outputs_nnet_large = generate_batch_song(\n",
    "                 #   self.sample_input, self.batch_song, start_index=i, fs=self.frame_per_second,\n",
    "                  #  seq_len=seq_len, use_tqdm=False) # We use the function that have been defined here\n",
    "                #inputs_nnet_large = np.array(self.note_tokenizer.transform(inputs_nnet_large), dtype=np.int32)\n",
    "                #outputs_nnet_large = np.array(self.note_tokenizer.transform(outputs_nnet_large), dtype=np.int32)\n",
    "\n",
    "                # EXAMPLE LARGE INPUTS = ARRAY([1,2,3,4],[2,3,4,5],[2,3,4,5],[2,3,4,5],[1,2,3,4])\n",
    "                input_batch = [y for x in self.sample_train[i:i+self.batch_song] for y in x]\n",
    "                output_batch = [y for x in self.sample_target[i:i+self.batch_song] for y in x]\n",
    "                c = list(zip(input_batch, output_batch))\n",
    "                sample_in = sample(c, 10000)\n",
    "                input_batch, output_batch = zip(*sample_in)\n",
    "                inputs_nnet_large = np.array(input_batch)\n",
    "                outputs_nnet_large = np.array(output_batch)\n",
    "\n",
    "                # Get an index of all windows in a song\n",
    "                index_shuffled = np.arange(start=0, stop=len(inputs_nnet_large))\n",
    "                np.random.shuffle(index_shuffled)\n",
    "\n",
    "                for nnet_steps in tqdm_notebook(range(0,len(index_shuffled),self.batch_nnet_size)):\n",
    "                    steps_nnet += 1\n",
    "                    current_index = index_shuffled[nnet_steps:nnet_steps+self.batch_nnet_size]\n",
    "\n",
    "                    inputs_nnet, outputs_nnet = inputs_nnet_large[current_index], outputs_nnet_large[current_index]\n",
    "\n",
    "                    # To make sure no exception thrown by tensorflow on autograph\n",
    "                    if len(inputs_nnet) // self.batch_nnet_size != 1:\n",
    "                        break\n",
    "                    loss = self.train_step(inputs_nnet, outputs_nnet)\n",
    "                    loss_total += tf.math.reduce_sum(loss)\n",
    "                    if steps_nnet % 20 == 0:\n",
    "                        print(\"epochs {} | Steps {} | total loss : {}\".format(epoch + 1, steps_nnet,loss_total))\n",
    "\n",
    "                    #checkpoint.save(file_prefix = self.checkpoint_prefix)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, inputs, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self.model(inputs)\n",
    "            loss = self.loss_fn(targets, prediction)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "EPOCHS = 2\n",
    "BATCH_SONG = 10\n",
    "BATCH_NNET_SIZE = 96\n",
    "TOTAL_SONGS = len(unique_target)\n",
    "FRAME_PER_SECOND = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:285: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4563f386a0434f4a927619e7f3f52f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=2.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:294: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1221dc95e644518cc207add32d9255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='MUSIC', max=7.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:316: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d98ce54ba284bf7bcbdb6a5d5544a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TrainModel.train_step of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fdcc0cfa748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TrainModel.train_step of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fdcc0cfa748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method TrainModel.train_step of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fdcc0cfa748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TrainModel.train_step of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fdcc0cfa748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "epochs 1 | Steps 20 | total loss : Tensor(\"add_4179:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 40 | total loss : Tensor(\"add_4199:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 60 | total loss : Tensor(\"add_4219:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 80 | total loss : Tensor(\"add_4239:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 100 | total loss : Tensor(\"add_4259:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:316: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae94f801c9b64320a5bf92f7b6966213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 | Steps 120 | total loss : Tensor(\"add_4278:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 140 | total loss : Tensor(\"add_4298:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 160 | total loss : Tensor(\"add_4318:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 180 | total loss : Tensor(\"add_4338:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 200 | total loss : Tensor(\"add_4358:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a721341647a471b92621e166165bdaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 | Steps 220 | total loss : Tensor(\"add_4377:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 240 | total loss : Tensor(\"add_4397:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 260 | total loss : Tensor(\"add_4417:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 280 | total loss : Tensor(\"add_4437:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 300 | total loss : Tensor(\"add_4457:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0753bcf9b1a745eab35b20e770c1f783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 | Steps 320 | total loss : Tensor(\"add_4476:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 340 | total loss : Tensor(\"add_4496:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 360 | total loss : Tensor(\"add_4516:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 380 | total loss : Tensor(\"add_4536:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 400 | total loss : Tensor(\"add_4556:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8335e8d9eb544f481ee2787e3a7a923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 | Steps 440 | total loss : Tensor(\"add_4595:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 460 | total loss : Tensor(\"add_4615:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 480 | total loss : Tensor(\"add_4635:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 500 | total loss : Tensor(\"add_4655:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 520 | total loss : Tensor(\"add_4675:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d643f9adaf234ffe97e584dec7deb5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 | Steps 540 | total loss : Tensor(\"add_4694:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 560 | total loss : Tensor(\"add_4714:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 580 | total loss : Tensor(\"add_4734:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 600 | total loss : Tensor(\"add_4754:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 620 | total loss : Tensor(\"add_4774:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ca9abe0a044f808b900b82ed1b7bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 | Steps 640 | total loss : Tensor(\"add_4793:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 660 | total loss : Tensor(\"add_4813:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 680 | total loss : Tensor(\"add_4833:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 700 | total loss : Tensor(\"add_4853:0\", shape=(), dtype=float32)\n",
      "epochs 1 | Steps 720 | total loss : Tensor(\"add_4873:0\", shape=(), dtype=float32)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:294: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c3fe5c33324adc9a6410532757899b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='MUSIC', max=7.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4afd28cb7b45b7b43645618066b8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 20 | total loss : Tensor(\"add_4907:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 40 | total loss : Tensor(\"add_4927:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 60 | total loss : Tensor(\"add_4947:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 80 | total loss : Tensor(\"add_4967:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 100 | total loss : Tensor(\"add_4987:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f7d217e3804acca31958dfd3f74a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 120 | total loss : Tensor(\"add_5006:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 140 | total loss : Tensor(\"add_5026:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 160 | total loss : Tensor(\"add_5046:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 180 | total loss : Tensor(\"add_5066:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 200 | total loss : Tensor(\"add_5086:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1957cd5090024916a4498fe7750eb651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 220 | total loss : Tensor(\"add_5105:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 240 | total loss : Tensor(\"add_5125:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 260 | total loss : Tensor(\"add_5145:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 280 | total loss : Tensor(\"add_5165:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 300 | total loss : Tensor(\"add_5185:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18875e6e617e4c279985c9d8426cd55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 320 | total loss : Tensor(\"add_5204:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 340 | total loss : Tensor(\"add_5224:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 360 | total loss : Tensor(\"add_5244:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 380 | total loss : Tensor(\"add_5264:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 400 | total loss : Tensor(\"add_5284:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6213129fc21e42e193a6efbd61b6d4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 440 | total loss : Tensor(\"add_5323:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 460 | total loss : Tensor(\"add_5343:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 480 | total loss : Tensor(\"add_5363:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 500 | total loss : Tensor(\"add_5383:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 520 | total loss : Tensor(\"add_5403:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924b62bbceec40989df177717c30bf4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 540 | total loss : Tensor(\"add_5422:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 560 | total loss : Tensor(\"add_5442:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 580 | total loss : Tensor(\"add_5462:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 600 | total loss : Tensor(\"add_5482:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 620 | total loss : Tensor(\"add_5502:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec31def47f2c450cb8882ecb9f975a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 640 | total loss : Tensor(\"add_5521:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 660 | total loss : Tensor(\"add_5541:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 680 | total loss : Tensor(\"add_5561:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 700 | total loss : Tensor(\"add_5581:0\", shape=(), dtype=float32)\n",
      "epochs 2 | Steps 720 | total loss : Tensor(\"add_5601:0\", shape=(), dtype=float32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_class = TrainModel(EPOCHS, unique_train, unique_target, BATCH_NNET_SIZE, BATCH_SONG, optimizer, checkpoint, loss_fn, checkpoint_prefix, \n",
    "                        TOTAL_SONGS, model)\n",
    "train_class.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('chopin_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
